{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUNEF MUCD 2021/2022  \n",
    "## Machine Learning\n",
    "## Análisis de Siniestralidad de Automóviles\n",
    "\n",
    "### Autores:\n",
    "- Andrés Mahía Morado\n",
    "- Antonio Tello Gómez\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autotime extension is already loaded. To reload it, use:\n",
      "  %reload_ext autotime\n",
      "time: 0 ns (started: 2021-12-18 18:46:33 +01:00)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, \\\n",
    "                            silhouette_score, recall_score, precision_score, make_scorer, \\\n",
    "                            roc_auc_score, f1_score, precision_recall_curve\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, \\\n",
    "                            classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from aux_func import evaluate_model, cargar_modelo\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.88 s (started: 2021-12-18 18:42:34 +01:00)\n"
     ]
    }
   ],
   "source": [
    "xtrain = pd.read_parquet(\"../data/xtrain.parquet\")\n",
    "ytrain = pd.read_parquet(\"../data/ytrain.parquet\")['fatality']\n",
    "xtest = pd.read_parquet(\"../data/xtest.parquet\")\n",
    "ytest = pd.read_parquet(\"../data/ytest.parquet\")['fatality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 172 ms (started: 2021-12-18 18:42:36 +01:00)\n"
     ]
    }
   ],
   "source": [
    "#Cargamos pipeline preprocesado\n",
    "preprocessor = cargar_modelo('../models/preprocessor.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El XGBoost o extreme gradient boosting es un procedimiento para el ensamblado de algoritmos de Machine Learning. Sus siglas hacen referencia a eXtreme Gradient Boosting. Este método, al contrario que el bagging, no procesa en paralelo los modelos, es más complejo.\n",
    "\n",
    "El procedimiento es secuencial. Se empieza con un modelo, que entrena con unos datos, y se evalua con un conjunto de test. Una vez evaluado el desempeño de ese primer modelo, se localizan aquellas observaciones en las que el modelo ha tenido más problemas y ha fallado. Estas observaciones tienen más probabilidad de aparecer en los datos para el siguiente modelo, y así sucesivamente. Lo que busca es pulir una y otra vez las observaciones que son difíciles de clasificar.\n",
    "\n",
    "Es un método potente y complejo, flexible en el sentido de que los modelos no tienen por qué ser del mismo tipo.\n",
    "\n",
    "Procedemos a entrenar un modelo de XGBoost con nuestros datos y parámetros por defecto.\n",
    "\n",
    "![Highway](https://programmerclick.com/images/517/897f66771ca1b47c9ed91516b58a621d.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2021-12-18 18:42:42 +01:00)\n"
     ]
    }
   ],
   "source": [
    "clf = Pipeline(steps=[\n",
    "    ('preprocesador', preprocessor),\n",
    "    ('clasificador', XGBClassifier(n_jobs=-1, random_state=0))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:43:15] WARNING: D:\\bld\\xgboost-split_1637426510059\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocesador',\n",
       "                 ColumnTransformer(transformers=[('num',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   SimpleImputer(strategy='median')),\n",
       "                                                                  ('scaler',\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  ['vehicle_age',\n",
       "                                                   'passenger_age',\n",
       "                                                   'vehicles_involved',\n",
       "                                                   'year']),\n",
       "                                                 ('fcat',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   SimpleImputer(fill_value=nan,\n",
       "                                                                                 strategy='constant')),\n",
       "                                                                  ('onehot',\n",
       "                                                                   OneHotEncoder(handle_unknown='i...\n",
       "                               gamma=0, gpu_id=-1, importance_type=None,\n",
       "                               interaction_constraints='',\n",
       "                               learning_rate=0.300000012, max_delta_step=0,\n",
       "                               max_depth=6, min_child_weight=1, missing=nan,\n",
       "                               monotone_constraints='()', n_estimators=100,\n",
       "                               n_jobs=-1, num_parallel_tree=1, predictor='auto',\n",
       "                               random_state=0, reg_alpha=0, reg_lambda=1,\n",
       "                               scale_pos_weight=1, subsample=1,\n",
       "                               tree_method='exact', validate_parameters=1,\n",
       "                               verbosity=None))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3min 2s (started: 2021-12-18 18:42:46 +01:00)\n"
     ]
    }
   ],
   "source": [
    "clf.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 31 ms (started: 2021-12-18 18:45:49 +01:00)\n"
     ]
    }
   ],
   "source": [
    "with open('../models/XGBoost.pickle', 'wb') as f:\n",
    "    pickle.dump(clf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 16 ms (started: 2021-12-18 18:45:49 +01:00)\n"
     ]
    }
   ],
   "source": [
    "with open('../models/XGBoost.pickle', 'rb') as f:\n",
    "    clf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generamos las predicciones sobre los datos de validación y evaluamos el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC score of the model: 0.8541541641828294\n",
      "Accuracy of the model: 0.9848714106843199\n",
      "\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99    799946\n",
      "           1       0.50      0.01      0.03     12291\n",
      "\n",
      "    accuracy                           0.98    812237\n",
      "   macro avg       0.74      0.51      0.51    812237\n",
      "weighted avg       0.98      0.98      0.98    812237\n",
      "\n",
      "\n",
      "Confusion matrix: \n",
      "[[799774    172]\n",
      " [ 12116    175]]\n",
      "\n",
      "time: 8.31 s (started: 2021-12-18 18:45:49 +01:00)\n"
     ]
    }
   ],
   "source": [
    "ypred = clf.predict(xtest)\n",
    "ypred_proba = clf.predict_proba(xtest)\n",
    "evaluate_model(ytest,ypred,ypred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajuste del umbral de predicción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De la misma manera que ocurría en el modelo Random Forest, nuestro modelo obtiene un valor de recall muy bajo para la clase minoritaria.\n",
    "Procedemos a ajustar el umbral de predicción siguiendo el punto óptimo según la curva ROC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold=0.014962, G-Mean=0.770\n",
      "ROC-AUC score of the model: 0.8541541641828294\n",
      "Accuracy of the model: 0.775515028249144\n",
      "\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.78      0.87    799946\n",
      "           1       0.05      0.77      0.09     12291\n",
      "\n",
      "    accuracy                           0.78    812237\n",
      "   macro avg       0.52      0.77      0.48    812237\n",
      "weighted avg       0.98      0.78      0.86    812237\n",
      "\n",
      "\n",
      "Confusion matrix: \n",
      "[[620499 179447]\n",
      " [  2888   9403]]\n",
      "\n",
      "time: 1.58 s (started: 2021-12-18 18:45:57 +01:00)\n"
     ]
    }
   ],
   "source": [
    "# keep probabilities for the positive outcome only\n",
    "yhat = ypred_proba[:, 1]\n",
    "# calculate roc curves\n",
    "fpr, tpr, thresholds = roc_curve(ytest, yhat)\n",
    "\n",
    "gmeans = np.sqrt(tpr * (1-fpr))\n",
    "# locate the index of the largest g-mean\n",
    "ix = np.argmax(gmeans)\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))\n",
    "\n",
    "ypred_new_threshold = (ypred_proba[:,1]>thresholds[ix]).astype(int)\n",
    "evaluate_model(ytest,ypred_new_threshold,ypred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar como el ajuste del threshold dota al modelo de un mayor recall para los casos de la clase minoritaria, lo cual nos interesa desde un punto de vista práctico a pesar de reducir la precisión y accuracy del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprobación de overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobamos si el modelo sufre de overfitting, realizando una predicción sobre la serie de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold=0.015049, G-Mean=0.782\n",
      "ROC-AUC score of the model: 0.8666278527727284\n",
      "Accuracy of the model: 0.7778137210694549\n",
      "\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.78      0.87   3200049\n",
      "           1       0.05      0.79      0.10     48896\n",
      "\n",
      "    accuracy                           0.78   3248945\n",
      "   macro avg       0.52      0.78      0.48   3248945\n",
      "weighted avg       0.98      0.78      0.86   3248945\n",
      "\n",
      "\n",
      "Confusion matrix: \n",
      "[[2488656  711393]\n",
      " [  10478   38418]]\n",
      "\n",
      "time: 34.2 s (started: 2021-12-18 18:45:59 +01:00)\n"
     ]
    }
   ],
   "source": [
    "ypred = clf.predict(xtrain)\n",
    "ypred_proba = clf.predict_proba(xtrain)\n",
    "\n",
    "# keep probabilities for the positive outcome only\n",
    "yhat = ypred_proba[:, 1]\n",
    "# calculate roc curves\n",
    "fpr, tpr, thresholds = roc_curve(ytrain, yhat)\n",
    "\n",
    "gmeans = np.sqrt(tpr * (1-fpr))\n",
    "# locate the index of the largest g-mean\n",
    "ix = np.argmax(gmeans)\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))\n",
    "\n",
    "ypred_new_threshold = (ypred_proba[:,1]>thresholds[ix]).astype(int)\n",
    "evaluate_model(ytrain,ypred_new_threshold,ypred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A diferencia del modelo Random Forest, XGBoost no ha realizado un ajuste tan pronunciado sobre los datos de entrenamiento.\n",
    "Esto puede deberse a la naturaleza y proceso interno de los algoritmos, ya que el modelo Random Forest gana robustez cuanto más profundo y optimizado está.\n",
    "Sin embargo, el modelo XGBoost tiende al overfitting cuando aumentamos el poder de computación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost con SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prueba de aplicación del mismo modelo a la versión de los datos que incluye Oversampling de la clase minoritaria mediante SMOTE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2021-12-18 18:48:14 +01:00)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.pipeline import Pipeline\n",
    "clf = Pipeline(steps=[\n",
    "    ('preprocesador', preprocessor),\n",
    "    ('smote', SMOTE(sampling_strategy=0.4, n_jobs=-1)),\n",
    "    ('clasificador', XGBClassifier(n_jobs=-1, random_state=0))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:49:36] WARNING: D:\\bld\\xgboost-split_1637426510059\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocesador',\n",
       "                 ColumnTransformer(transformers=[('num',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   SimpleImputer(strategy='median')),\n",
       "                                                                  ('scaler',\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  ['vehicle_age',\n",
       "                                                   'passenger_age',\n",
       "                                                   'vehicles_involved',\n",
       "                                                   'year']),\n",
       "                                                 ('fcat',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   SimpleImputer(fill_value=nan,\n",
       "                                                                                 strategy='constant')),\n",
       "                                                                  ('onehot',\n",
       "                                                                   OneHotEncoder(handle_unknown='i...\n",
       "                               gamma=0, gpu_id=-1, importance_type=None,\n",
       "                               interaction_constraints='',\n",
       "                               learning_rate=0.300000012, max_delta_step=0,\n",
       "                               max_depth=6, min_child_weight=1, missing=nan,\n",
       "                               monotone_constraints='()', n_estimators=100,\n",
       "                               n_jobs=-1, num_parallel_tree=1, predictor='auto',\n",
       "                               random_state=0, reg_alpha=0, reg_lambda=1,\n",
       "                               scale_pos_weight=1, subsample=1,\n",
       "                               tree_method='approx', validate_parameters=1,\n",
       "                               verbosity=None))])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 7min 14s (started: 2021-12-18 18:48:19 +01:00)\n"
     ]
    }
   ],
   "source": [
    "clf.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 63 ms (started: 2021-12-18 18:55:33 +01:00)\n"
     ]
    }
   ],
   "source": [
    "with open('../models/XGBoost_smote.pickle', 'wb') as f:\n",
    "    pickle.dump(clf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 31 ms (started: 2021-12-18 18:55:33 +01:00)\n"
     ]
    }
   ],
   "source": [
    "with open('../models/XGBoost_smote.pickle', 'rb') as f:\n",
    "    clf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC score of the model: 0.8335443595477434\n",
      "Accuracy of the model: 0.9847039718702792\n",
      "\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99    799946\n",
      "           1       0.35      0.01      0.02     12291\n",
      "\n",
      "    accuracy                           0.98    812237\n",
      "   macro avg       0.67      0.51      0.51    812237\n",
      "weighted avg       0.98      0.98      0.98    812237\n",
      "\n",
      "\n",
      "Confusion matrix: \n",
      "[[799656    290]\n",
      " [ 12134    157]]\n",
      "\n",
      "time: 8.98 s (started: 2021-12-18 18:55:33 +01:00)\n"
     ]
    }
   ],
   "source": [
    "ypred = clf.predict(xtest)\n",
    "ypred_proba = clf.predict_proba(xtest)\n",
    "evaluate_model(ytest,ypred,ypred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo sin aplicar el ajuste de threshold no es lo suficientemente descriptivo como para poder compararlo con su versión análoga. Procedemos a realizar dicho ajuste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajuste del umbral de predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold=0.022001, G-Mean=0.750\n",
      "ROC-AUC score of the model: 0.8335443595477434\n",
      "Accuracy of the model: 0.74873589851238\n",
      "\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.75      0.85    799946\n",
      "           1       0.04      0.75      0.08     12291\n",
      "\n",
      "    accuracy                           0.75    812237\n",
      "   macro avg       0.52      0.75      0.47    812237\n",
      "weighted avg       0.98      0.75      0.84    812237\n",
      "\n",
      "\n",
      "Confusion matrix: \n",
      "[[598906 201040]\n",
      " [  3046   9245]]\n",
      "\n",
      "time: 1.66 s (started: 2021-12-18 18:55:42 +01:00)\n"
     ]
    }
   ],
   "source": [
    "# keep probabilities for the positive outcome only\n",
    "yhat = ypred_proba[:, 1]\n",
    "# calculate roc curves\n",
    "fpr, tpr, thresholds = roc_curve(ytest, yhat)\n",
    "\n",
    "gmeans = np.sqrt(tpr * (1-fpr))\n",
    "ix = np.argmax(gmeans)\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))\n",
    "\n",
    "ypred_new_threshold = (ypred_proba[:,1]>thresholds[ix]).astype(int)\n",
    "evaluate_model(ytest,ypred_new_threshold,ypred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tras realizar el ajuste del modelo, podemos afirmar que en nuestro caso concreto y para el tratamiento de los datos que hemos realizado, no existe una razón por la que usar la versión Oversample de los datos.\n",
    "Los resultados obtenidos son peores respecto del modelo generado con datos sin alterar."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "75234cef4d4b5401408a5d3b72f4265569f6601e8da7c73f173a51e561ebd976"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('core_models': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
